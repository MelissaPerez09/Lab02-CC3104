{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b08a879",
   "metadata": {},
   "source": [
    "## Laboratorio 02 - MDP\n",
    "Integrantes:\n",
    "- Ricardo Méndez\n",
    "- Sara Echevería\n",
    "- Melissa Pérez\n",
    "\n",
    "Repositorio: https://github.com/MelissaPerez09/Lab02-CC3104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0847a12",
   "metadata": {},
   "source": [
    "### Task 01\n",
    "1. ¿Qué es un Markov Decision Process (MDP)?\n",
    "    - Es un modelo matemático donde un agente interactúa con el entorno, principalmente busca maximizar una recompensa acumulada. Se basa en la propiedad de Markov por lo que la toma de decisiones se basa en el estado actual y no en los estados pasados.\n",
    "2. ¿Cuáles son los componentes principales de un MDP?\n",
    "    - `S`: conjunto de estados posibles del entorno.\n",
    "    - `A`: conjunto de acciones que el agente puede tomar.\n",
    "    - `P(s'|s, a)`: función de transición de probabilidad, que describe la probabilidad de pasar al estado `s'` al tomar la acción a desde el estado `s`.\n",
    "    - `R(s, a)`: función de recompensa, que asigna un valor numérico (recompensa esperada) al tomar la acción a en el estado `s`.\n",
    "    - `γ (gamma)`: factor de descuento, un valor entre 0 y 1 que determina la importancia de las recompensas futuras frente a las inmediatas.\n",
    "3. ¿Cuál es el objetivo principal del aprendizaje por refuerzo con MDPs?\n",
    "    - Aprender una política óptima, que define qué acción tomar en cada estado para maximizar la recompensa acumulada a largo plazo. Esto se logra evaluando y mejorando políticas mediante métodos como Value Iteration, Policy Iteration, o algoritmos como Q-learning o SARSA.\n",
    "\n",
    "_Givan, Bob. An Introduction to Markov Decision Processes https://www.cs.rice.edu/~vardi/dag01/givan1.pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfb130",
   "metadata": {},
   "source": [
    "### Task 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e706a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e2027d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(range(9))\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "obstacles = [4,8]\n",
    "goal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23960e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    row, col = divmod(state, 3)\n",
    "\n",
    "    if action == 'up':\n",
    "        row = max(0, row - 1)\n",
    "    elif action == 'down':\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 'left':\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 'right':\n",
    "        col = min(2, col + 1)\n",
    "\n",
    "    next_state = row * 3 + col\n",
    "    \n",
    "    return next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3bd0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {s: {} for s in states}\n",
    "\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        next_state = get_next_state(s, a)\n",
    "        if a not in P[s]:\n",
    "            P[s][a] = {}\n",
    "        P[s][a][next_state] = 1.0 # Deterministic transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0d5d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {s: {} for s in states}\n",
    "\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        next_state = get_next_state(s, a)\n",
    "        if a not in R[s]:\n",
    "            R[s][a] = {}\n",
    "        if s == next_state and next_state in obstacles:\n",
    "            R[s][a][next_state] = -10 #Penalty for crashing into an obstacle\n",
    "        elif next_state == goal:\n",
    "            R[s][a][next_state] = 10\n",
    "        else:\n",
    "            R[s][a][next_state] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e901e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    0: 'right',\n",
    "    1: 'right',\n",
    "    2: 'down', #goal\n",
    "    3: 'right',\n",
    "    4: 'up',    #obstacle\n",
    "    5: 'down',\n",
    "    6: 'right',\n",
    "    7: 'right',\n",
    "    8: 'right'  #obstacle\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "361b3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(start_state, policy, P, R, steps=20):\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    path = [state]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        if state == goal:\n",
    "            break\n",
    "        action = policy[state]\n",
    "        transitions = P[state][action]\n",
    "        next_states = list(transitions.keys())\n",
    "        probabilities = list(transitions.values())\n",
    "        next_state = random.choices(next_states, probabilities)[0]\n",
    "        reward = R[state][action][next_state]\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        path.append(state)\n",
    "    return total_reward, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3ce4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(runs=100):\n",
    "    rewards = []\n",
    "    for _ in range(runs):\n",
    "        reward, path = simulate_policy(0, policy, P, R)\n",
    "        rewards.append(reward)\n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c8dd55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 runs: 9.0\n"
     ]
    }
   ],
   "source": [
    "avg_reward = evaluate_policy()\n",
    "print(f\"Average reward over 100 runs: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641568a6",
   "metadata": {},
   "source": [
    "### Task 03\n",
    "__En clase hemos dicho que una vez tengamos v* o q* sabemos la póliza óptima π* ¿Por qué?__\n",
    "\n",
    "Porque una vez se tiene `v*` o `q*`, ya no se necesita seguir explorando el entorno. Esto se debe a que:\n",
    "- `v*` es el valor máximo que se puede obtener desde un estado `s` siguiendo la mejor política posible.\n",
    "- `q*` es el valor máximo esperado si el agente toma la acción en el estado `s`y luego sigue la mejor política posible.\n",
    "\n",
    "**CREO QUE HAY QUE AGREGAR MÁS**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
